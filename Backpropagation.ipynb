{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is fundamental to how neural networks learn. At a high level, we take the error and feed it back into the network, updating the weights.\n",
    "\n",
    "To update the weights to hidden layers using gradient descent, you need to know how much error each of the hidden units contributed to the final output. Since the output of a layer is determined by the weights between layers, the error resulting from units is scaled by the weights going forward through the network. Since we know the error at the output, we can use the weights to work backwards to hidden layers.\n",
    "\n",
    "For example, in the output layer, you have errors $\\delta_k^o$ attributed to each output unit k. Then, the error attributed to hidden unit j is the output errors, scaled by the weights between the output and hidden layers (and the gradient):\n",
    "\n",
    "$$\n",
    "\\delta_h^j = \\sum W_{jk} \\delta_k^o f'(h_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the gradient descent step is the same as before, just with the new errors:\n",
    "$$\n",
    "\\Delta w_{ij} = \\eta \\delta_j^h x_i\n",
    "$$\n",
    "where $w_{ij}$ are the weights between the inputs and hidden layer and $x_i$ are input unit values. The weight steps are equal to the step size times the output error of the layer times the values of the inputs to that layer.\n",
    "$$\n",
    "\\Delta w_{pq} = \\eta \\delta_{output} V_{in}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example. <img src=\"./backprop-network.png\" width=200 height=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Assume we're trying to fit some binary data and the target is $y = 1$. We'll start with the forward pass, first calculating the **input** to the hidden unit:\n",
    "\n",
    "$h = \\sum_i w_i x_i = 0.1 \\times 0.4 + 0.3 \\times -0.2 = -.02$\n",
    "\n",
    "And the **output** is:\n",
    "\n",
    "$a = f(h) = \\text{sigmoid}(-.02) = .495$\n",
    "\n",
    "Using this as the input to the output unit, the output of the network is:\n",
    "\n",
    "$\\hat{y} = f(W \\cdot a) = \\text{sigmoid}(0.1 \\times .495) = .512$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the network output, we can start the backwards pass to calculate the weight updates for both layers. Using the fact that for the sigmoid function $f'(W \\cdot a) = f(W \\cdot a)(1 - f(W \\cdot a))$, the error term for the output unit is:\n",
    "\n",
    "$\\delta^o = (y - \\hat{y})f'(W \\cdot a) = (1 - .512) \\times .512 \\times (1 - .512) = .122$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the error term for the hidden unit with backpropagation. Here we'll scale the error term from the output unit by the weight $W$ connecting it to the hidden unit. For the hidden unit error term, $\\delta_h^j = \\sum W_{jk} \\delta_k^o f'(h_j)$, but since we have one hidden unit and one output unit, this is much simpler.\n",
    "\n",
    "$\\delta^h = W \\delta^o f'(h) = .1 \\times .122 \\times .495 \\times (1 - .495) = .003$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
