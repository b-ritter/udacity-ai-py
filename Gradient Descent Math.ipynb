{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Math\n",
    "## Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./simple_neural_network.png\" width=500 height=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error\n",
    "\n",
    "$ E = (y - \\hat{y})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y$ is our actual value, $\\hat{y}$ is the prediction. Why take the square and not the absolute value? It makes all values positive. It also penalizes outliers more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is just for one point, however. For the entire data set, the error looks like this for each point $\\mu$ in the data set:\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{\\mu} (y^{\\mu} - \\hat{y}^{\\mu})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known as the **sum of the squared errors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\hat{y}$ is $f(\\sum_{i} w_ix_i^{\\mu})$ where $f(x)$ is the sigmoid function.\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{\\mu} (y^{\\mu} - f(\\sum_{i} w_ix_i^{\\mu}))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the weights on each pass like so $w_i = w_i + \\Delta w_i$ where $\\Delta$ is the negative of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights are proportional to the partial derivative of the error with respect to the weight.\n",
    "$$\n",
    "\\Delta w_i \\propto - \\frac{\\delta E}{\\delta w_i}\n",
    "$$\n",
    "\n",
    "We can also throw in the learning rate, $\\eta$\n",
    "$$\n",
    "\\Delta w_i \\propto - \\eta \\frac{\\delta E}{\\delta w_i}\n",
    "$$\n",
    "\n",
    "Which equals:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta E}{\\delta w_i}& = \\frac{\\delta}{\\delta w_i} \\frac{1}{2}(y - \\hat{y})^2 \\\\\n",
    "& =\\frac{\\delta}{\\delta w_i} \\frac{1}{2}(y - \\hat{y}(w_i))^2\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This requires the **chain rule** to solve. Quick refresher on the chain rule: \n",
    "\n",
    "$$\n",
    "\\frac{\\delta}{\\delta z} p(q(z)) = \\frac{\\delta p}{\\delta q}\\frac{\\delta q}{\\delta z}\n",
    "$$\n",
    "\n",
    "So in our problem, $p = \\frac{1}{2}q(w_i)^2$ and $q = y - \\hat{y}(w_i)$\n",
    "\n",
    "Then using the chain rule again:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta E}{\\delta w_i} & = (y - \\hat{y}) \\frac{\\delta}{\\delta w_i} (y - \\hat{y}) \\\\\n",
    "& = (y - \\hat{y}) \\frac{\\delta}{\\delta w_i} (y - \\hat{y}) \\\\\n",
    "& = -(y - \\hat{y}) \\frac{\\delta \\hat{y}}{\\delta w_i} \n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "Recall that $\\hat{y} = f(h)$, where $f$ is the activation function (such as sigmoid) of h. And also recall that $h$ is the linear combination of the input values and the weights: \n",
    "$$\n",
    "h = \\sum_{i} w_i x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule again: \n",
    "$$\n",
    "\\frac{\\delta E}{\\delta w_i} = (y - \\hat{y}) f'(h) \\frac{\\delta}{\\delta w_i} \\sum_{i} w_i x_i\n",
    "$$\n",
    "\n",
    "In $\\sum_{i} w_i x_i$, only one term depends on each weight.\n",
    "\n",
    "So $$\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta w_i} \\sum_{i} w_i x_i & = \\frac{\\delta}{\\delta w_1} [w_1x_1 + w_2x_2 + \\dots + w_nx_n]\\\\\n",
    "& = x_1 + 0 + 0 + \\dots\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together:\n",
    "$$\n",
    "\\frac{\\delta E}{\\delta w_i} = (y - \\hat{y})f'(h)x_i\n",
    "$$\n",
    "\n",
    "To update the weights:\n",
    "$$\n",
    "\\Delta w_i = \\eta (y - \\hat{y})f'(h)x_i\n",
    "$$\n",
    "\n",
    "To make things easier, we can define an error term $\\delta$ as $(y - \\hat{y})f'(h)$ so updating the weights is just:\n",
    "$$\n",
    "\\Delta w_i = w_i + \\eta \\delta x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are multiple output units:\n",
    "\n",
    "$$\n",
    "\\delta_j = \\eta(y_j - \\hat{y_j})f'(h_j)\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} = \\eta \\delta_j x_i\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
